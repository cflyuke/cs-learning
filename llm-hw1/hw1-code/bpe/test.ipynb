{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b10139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has already saved into bpe.model.\n"
     ]
    }
   ],
   "source": [
    "from bpe import Tokenizer\n",
    "text = open('manual.txt', 'r', encoding=\"utf-8\").read()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.train(text, 1024, verbose=False)\n",
    "model_file, vocab_file = tokenizer.save('bpe')\n",
    "print(f\"Tokenizer has already saved into {model_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a72d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactly the same!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(model_file)\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "if decoded == text:\n",
    "    print(\"Exactly the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d0eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.load(model_file)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "sentence1 = \"Originated as the Imperial University of Peking in 1898, Peking University was China’s first national comprehensive university and the supreme education authority at the time. Since the founding of the People’s Republic of China in 1949, it has developed into a comprehensive university with fundamental education and research in both humanities and science. The reform and opening-up of China in 1978 has ushered in a new era for the University unseen in history. And its merger with Beijing Medical University in 2000 has geared itself up for all-round and vibrant growth in such fields as science, engineering, medicine, agriculture, humanities and social sciences. Supported by the “211 Project” and the “985 Project”, the University has made remarkable achievements, such as optimizing disciplines, cultivating talents, recruiting high-caliber teachers, as well as teaching and scientific research, which paves the way for a world-class university.\"\n",
    "sentence2 = \"博士学位论文应当表明作者具有独立从事科学研究工作的能力，并在科学或专门技术上做出创造性的成果。博士学位论文或摘要，应当在答辩前三个月印送有关单位，并经同行评议。学位授予单位应当聘请两位与论文有关学科的专家评阅论文，其中一位应当是外单位的专家。评阅人应当对论文写详细的学术评语，供论文答辩委员会参考。\"\n",
    "def get_result(sentence):\n",
    "    gpt2_encoded = gpt2_tokenizer.encode(sentence)\n",
    "    my_encoded = my_tokenizer.encode(sentence)\n",
    "    gpt2_result = []\n",
    "    my_result = []\n",
    "    for idx in gpt2_encoded:\n",
    "        gpt2_result.append(gpt2_tokenizer.decode([idx]))\n",
    "    for idx in my_encoded:\n",
    "        my_result.append(my_tokenizer.decode([idx]))\n",
    "    return gpt2_result, my_result\n",
    "\n",
    "gpt2_result_1, my_result_1 = get_result(sentence1)\n",
    "gpt2_result_2, my_result_2 = get_result(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence1:\n",
      "type | length  | result\n",
      "gpt2 | 185     | ['Orig', 'inated', ' as', ' the', ' Imperial', ' University', ' of', ' P', 'eking', ' in', ' 1898', ',', ' P', 'eking', ' University', ' was', ' China', '�', '�', 's', ' first', ' national', ' comprehensive', ' university', ' and', ' the', ' supreme', ' education', ' authority', ' at', ' the', ' time', '.', ' Since', ' the', ' founding', ' of', ' the', ' People', '�', '�', 's', ' Republic', ' of', ' China', ' in', ' 1949', ',', ' it', ' has', ' developed', ' into', ' a', ' comprehensive', ' university', ' with', ' fundamental', ' education', ' and', ' research', ' in', ' both', ' humanities', ' and', ' science', '.', ' The', ' reform', ' and', ' opening', '-', 'up', ' of', ' China', ' in', ' 1978', ' has', ' ushered', ' in', ' a', ' new', ' era', ' for', ' the', ' University', ' unseen', ' in', ' history', '.', ' And', ' its', ' merger', ' with', ' Beijing', ' Medical', ' University', ' in', ' 2000', ' has', ' geared', ' itself', ' up', ' for', ' all', '-', 'round', ' and', ' vibrant', ' growth', ' in', ' such', ' fields', ' as', ' science', ',', ' engineering', ',', ' medicine', ',', ' agriculture', ',', ' humanities', ' and', ' social', ' sciences', '.', ' Supported', ' by', ' the', ' �', '�', '211', ' Project', '�', '�', ' and', ' the', ' �', '�', '985', ' Project', '�', '�', ',', ' the', ' University', ' has', ' made', ' remarkable', ' achievements', ',', ' such', ' as', ' optimizing', ' disciplines', ',', ' cultivating', ' talents', ',', ' recruiting', ' high', '-', 'caliber', ' teachers', ',', ' as', ' well', ' as', ' teaching', ' and', ' scientific', ' research', ',', ' which', ' p', 'aves', ' the', ' way', ' for', ' a', ' world', '-', 'class', ' university', '.']\n",
      "my   | 942     | ['O', 'r', 'i', 'g', 'i', 'n', 'a', 't', 'e', 'd', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 'I', 'm', 'p', 'e', 'r', 'i', 'a', 'l', ' ', 'U', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'o', 'f', ' P', 'e', 'k', 'i', 'n', 'g', ' ', 'i', 'n', ' 1', '8', '9', '8', ',', ' P', 'e', 'k', 'i', 'n', 'g', ' ', 'U', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'w', 'a', 's', ' ', 'C', 'h', 'i', 'n', 'a', '�', '�', 's', ' ', 'f', 'i', 'r', 's', 't', ' ', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'c', 'o', 'm', 'p', 'r', 'e', 'h', 'e', 'n', 's', 'i', 'v', 'e', ' ', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 's', 'u', 'p', 'r', 'e', 'm', 'e', ' ', 'e', 'd', 'u', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'u', 't', 'h', 'o', 'r', 'i', 't', 'y', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 't', 'i', 'm', 'e', '. ', 'S', 'i', 'n', 'c', 'e', ' ', 't', 'h', 'e', ' ', 'f', 'o', 'u', 'n', 'd', 'i', 'n', 'g', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' P', 'e', 'o', 'p', 'l', 'e', '�', '�', 's', ' ', 'R', 'e', 'p', 'u', 'b', 'l', 'i', 'c', ' ', 'o', 'f', ' ', 'C', 'h', 'i', 'n', 'a', ' ', 'i', 'n', ' 1', '9', '4', '9', ',', ' ', 'i', 't', ' ', 'h', 'a', 's', ' ', 'd', 'e', 'v', 'e', 'l', 'o', 'p', 'e', 'd', ' ', 'i', 'n', 't', 'o', ' ', 'a', ' ', 'c', 'o', 'm', 'p', 'r', 'e', 'h', 'e', 'n', 's', 'i', 'v', 'e', ' ', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'w', 'i', 't', 'h', ' ', 'f', 'u', 'n', 'd', 'a', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'e', 'd', 'u', 'c', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', ' ', 'i', 'n', ' ', 'b', 'o', 't', 'h', ' ', 'h', 'u', 'm', 'a', 'n', 'i', 't', 'i', 'e', 's', ' ', 'a', 'n', 'd', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', '. ', 'T', 'h', 'e', ' ', 'r', 'e', 'f', 'o', 'r', 'm', ' ', 'a', 'n', 'd', ' ', 'o', 'p', 'e', 'n', 'i', 'n', 'g', '-', 'u', 'p', ' ', 'o', 'f', ' ', 'C', 'h', 'i', 'n', 'a', ' ', 'i', 'n', ' 1', '9', '7', '8', ' ', 'h', 'a', 's', ' ', 'u', 's', 'h', 'e', 'r', 'e', 'd', ' ', 'i', 'n', ' ', 'a', ' ', 'n', 'e', 'w', ' ', 'e', 'r', 'a', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'U', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'u', 'n', 's', 'e', 'e', 'n', ' ', 'i', 'n', ' ', 'h', 'i', 's', 't', 'o', 'r', 'y', '. ', 'A', 'n', 'd', ' ', 'i', 't', 's', ' ', 'm', 'e', 'r', 'g', 'e', 'r', ' ', 'w', 'i', 't', 'h', ' ', 'B', 'e', 'i', 'j', 'i', 'n', 'g', ' ', 'M', 'e', 'd', 'i', 'c', 'a', 'l', ' ', 'U', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'i', 'n', ' 2', '0', '0', '0', ' ', 'h', 'a', 's', ' ', 'g', 'e', 'a', 'r', 'e', 'd', ' ', 'i', 't', 's', 'e', 'l', 'f', ' ', 'u', 'p', ' ', 'f', 'o', 'r', ' ', 'a', 'l', 'l', '-', 'r', 'o', 'u', 'n', 'd', ' ', 'a', 'n', 'd', ' ', 'v', 'i', 'b', 'r', 'a', 'n', 't', ' ', 'g', 'r', 'o', 'w', 't', 'h', ' ', 'i', 'n', ' ', 's', 'u', 'c', 'h', ' ', 'f', 'i', 'e', 'l', 'd', 's', ' ', 'a', 's', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', ',', ' ', 'e', 'n', 'g', 'i', 'n', 'e', 'e', 'r', 'i', 'n', 'g', ',', ' ', 'm', 'e', 'd', 'i', 'c', 'i', 'n', 'e', ',', ' ', 'a', 'g', 'r', 'i', 'c', 'u', 'l', 't', 'u', 'r', 'e', ',', ' ', 'h', 'u', 'm', 'a', 'n', 'i', 't', 'i', 'e', 's', ' ', 'a', 'n', 'd', ' ', 's', 'o', 'c', 'i', 'a', 'l', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', 's', '. ', 'S', 'u', 'p', 'p', 'o', 'r', 't', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'e', ' ', '“', '2', '1', '1', ' P', 'r', 'o', 'j', 'e', 'c', 't', '”', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', '“', '9', '8', '5', ' P', 'r', 'o', 'j', 'e', 'c', 't', '”', ',', ' ', 't', 'h', 'e', ' ', 'U', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'h', 'a', 's', ' ', 'm', 'a', 'd', 'e', ' ', 'r', 'e', 'm', 'a', 'r', 'k', 'a', 'b', 'l', 'e', ' ', 'a', 'c', 'h', 'i', 'e', 'v', 'e', 'm', 'e', 'n', 't', 's', ',', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'o', 'p', 't', 'i', 'm', 'i', 'z', 'i', 'n', 'g', ' ', 'd', 'i', 's', 'c', 'i', 'p', 'l', 'i', 'n', 'e', 's', ',', ' ', 'c', 'u', 'l', 't', 'i', 'v', 'a', 't', 'i', 'n', 'g', ' ', 't', 'a', 'l', 'e', 'n', 't', 's', ',', ' ', 'r', 'e', 'c', 'r', 'u', 'i', 't', 'i', 'n', 'g', ' ', 'h', 'i', 'g', 'h', '-', 'c', 'a', 'l', 'i', 'b', 'e', 'r', ' ', 't', 'e', 'a', 'c', 'h', 'e', 'r', 's', ',', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', 't', 'e', 'a', 'c', 'h', 'i', 'n', 'g', ' ', 'a', 'n', 'd', ' ', 's', 'c', 'i', 'e', 'n', 't', 'i', 'f', 'i', 'c', ' ', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'p', 'a', 'v', 'e', 's', ' ', 't', 'h', 'e', ' ', 'w', 'a', 'y', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'w', 'o', 'r', 'l', 'd', '-', 'c', 'l', 'a', 's', 's', ' ', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', '.']\n",
      "\n",
      "Sentence2:\n",
      "type | length  | result\n",
      "gpt2 | 306     | ['�', '�', '士', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '作', '者', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '作', '的', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '上', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '的', '�', '�', '�', '�', '�', '。', '�', '�', '士', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '��', '�', '�', '�', '�', '�', '三', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '。', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '的', '�', '�', '�', '�', '�', '�', '�', '��', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '中', '一', '�', '�', '�', '�', '�', '�', '是', '�', '�', '�', '�', '�', '�', '的', '�', '�', '�', '�', '。', '�', '�', '�', '��', '人', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '的', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '��', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '。']\n",
      "my   | 118     | ['博士', '学位论文', '应当', '表', '明', '作', '者', '具', '有', '�', '�', '�', '立', '从', '事', '科学', '研究', '工作', '的', '能力', '，并', '在', '科学', '或', '专', '门', '�', '�', '�', '�', '上', '做', '出', '创', '造', '性', '的', '成果', '。', '博士', '学位论文', '或', '�', '�', '�', '要', '，', '应当', '在', '答辩', '前', '三', '个', '月', '�', '�', '送', '有关', '单位', '，并', '经', '同', '行', '评', '议', '。', '学位授予', '单位', '应当', '�', '�', '�', '�', '�', '�', '位', '与', '论文', '有关', '学科', '的', '专家', '评阅', '论文', '，', '其', '中', '一', '位', '应当', '是', '外', '单位', '的', '专家', '。', '评阅', '人', '应当', '对', '论文', '写', '�', '�', '细', '的', '学术', '评', '语', '，', '�', '�', '论文', '答辩', '委员会', '参', '考', '。']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Sentence1:\n",
    "type | length  | result\n",
    "gpt2 | {len(gpt2_result_1)}     | {gpt2_result_1}\n",
    "my   | {len(my_result_1)}     | {my_result_1}\"\"\")\n",
    "print(f\"\"\"\n",
    "Sentence2:\n",
    "type | length  | result\n",
    "gpt2 | {len(gpt2_result_2)}     | {gpt2_result_2}\n",
    "my   | {len(my_result_2)}     | {my_result_2}\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
