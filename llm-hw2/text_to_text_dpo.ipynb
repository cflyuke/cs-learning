{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9001b0ba",
   "metadata": {},
   "source": [
    "# 使用Align-Anything框架进行文本到文本的DPO训练\n",
    "\n",
    "这个教程介绍如何使用Align-Anything框架对文本模型进行直接偏好优化(DPO)训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afac3c7",
   "metadata": {},
   "source": [
    "# 准备工作\n",
    "\n",
    "- Align-Anything已安装。\n",
    "- 一个文本到文本偏好数据集，在本教程中，我们使用align_anything_t2t数据集。\n",
    "- Qwen2.5-0.5B-Instruct模型，已经下载到本地。\n",
    "- 一个至少有8GB内存的GPU。\n",
    "\n",
    "> DPO训练需要同时加载策略模型和参考模型，因此内存需求较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ff90a",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8ce177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from align_anything.models.pretrained_model import load_pretrained_models\n",
    "from align_anything.datasets.text_to_text.preference import PreferenceDataset\n",
    "from align_anything.configs.template import ChatTemplate\n",
    "from align_anything.utils.multi_process import get_current_device\n",
    "from align_anything.utils.tools import gather_log_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011af1c",
   "metadata": {},
   "source": [
    "## 加载预训练模型\n",
    "\n",
    "DPO训练需要两个模型：\n",
    "1. **策略模型(Policy Model)**: 需要训练的模型\n",
    "2. **参考模型(Reference Model)**: 用于计算KL散度的固定模型\n",
    "\n",
    "我们将使用Qwen2.5-0.5B-Instruct模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7451e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/align-anything/align_anything/models/pretrained_model.py:309: RuntimeWarning: The tokenizer vocabulary size (151665) is different from the model embedding size (151936) before resizing.\n",
      "  resize_tokenizer_embedding(tokenizer=tokenizer, model=model)\n",
      "/root/autodl-tmp/align-anything/align_anything/models/pretrained_model.py:309: RuntimeWarning: The tokenizer vocabulary size (151665) is different from the model embedding size (151936) after resizing.\n",
      "  resize_tokenizer_embedding(tokenizer=tokenizer, model=model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载到设备: cuda:0\n",
      "策略模型参数量: 494,032,768\n",
      "参考模型参数量: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "# 模型路径\n",
    "model_path = \"./Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载策略模型（需要训练的模型）\n",
    "policy_model, tokenizer, processor = load_pretrained_models(\n",
    "    model_path,\n",
    "    model_max_length=4096,\n",
    "    padding_side='left',  # DPO训练通常使用左填充\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 加载参考模型（固定不变的模型）\n",
    "reference_model, _, _ = load_pretrained_models(\n",
    "    model_path,\n",
    "    model_max_length=4096,\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 将模型移动到GPU\n",
    "device = get_current_device()\n",
    "policy_model = policy_model.to(device)\n",
    "reference_model = reference_model.to(device)\n",
    "\n",
    "# 参考模型设置为评估模式，不需要梯度\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"模型已加载到设备: {device}\")\n",
    "print(f\"策略模型参数量: {sum(p.numel() for p in policy_model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"参考模型参数量: {sum(p.numel() for p in reference_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a70e6",
   "metadata": {},
   "source": [
    "## 设置优化器\n",
    "\n",
    "DPO训练通常使用较小的学习率，因为我们是在已经训练好的模型基础上进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff39175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "优化器已初始化，学习率: 5e-07\n"
     ]
    }
   ],
   "source": [
    "# 初始化优化器，使用较小的学习率\n",
    "optimizer = AdamW(policy_model.parameters(), lr=5e-7, weight_decay=0.01)\n",
    "\n",
    "print(f\"优化器已初始化，学习率: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df663ae6",
   "metadata": {},
   "source": [
    "## 配置 Chat Template\n",
    "\n",
    "我们使用 HOMEWORK 模板来格式化偏好数据。这个模板专门为我们的数据集设计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c959fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Template 已配置\n"
     ]
    }
   ],
   "source": [
    "# 创建聊天模板\n",
    "train_template = ChatTemplate(\n",
    "    formatter=tokenizer,\n",
    "    template=\"HOMEWORK\",\n",
    ")\n",
    "\n",
    "print(\"Chat Template 已配置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66497835",
   "metadata": {},
   "source": [
    "## 创建偏好数据集\n",
    "\n",
    "偏好数据集包含问题和两个回答（更好的和更差的），用于DPO训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed2ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering valid indices: 100%|██████████| 1000/1000 [00:00<00:00, 3737.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集已加载，包含 1000 个样本\n",
      "\n",
      "数据样本示例:\n",
      "更好的对话: <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can a Muslim person use pork flavoring?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "No, a Muslim ...\n",
      "更差的对话: <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can a Muslim person use pork flavoring?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "No, a Muslim ...\n",
      "更好回答长度: 92\n",
      "更差回答长度: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 初始化训练数据集\n",
    "train_dataset = PreferenceDataset(\n",
    "    path=\"./align_anything_t2t\",  # 数据集路径\n",
    "    template=train_template,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    split=\"train\",\n",
    "    size=1000,  # 限制为1000个样本用于演示\n",
    ")\n",
    "\n",
    "print(f\"训练数据集已加载，包含 {len(train_dataset)} 个样本\")\n",
    "\n",
    "# 查看一个数据样本\n",
    "sample = train_dataset[0]\n",
    "print(\"\\n数据样本示例:\")\n",
    "print(f\"更好的对话: {sample['better_conversation'][:200]}...\")\n",
    "print(f\"更差的对话: {sample['worse_conversation'][:200]}...\")\n",
    "print(f\"更好回答长度: {sample['better_response_lens']}\")\n",
    "print(f\"更差回答长度: {sample['worse_response_lens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61afbe",
   "metadata": {},
   "source": [
    "## 设定 DataLoader\n",
    "\n",
    "DataLoader 将处理批量、打乱和加载偏好数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c469b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 已创建，批次大小: 1\n",
      "总批次数: 1000\n"
     ]
    }
   ],
   "source": [
    "# 创建训练数据加载器\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=train_dataset.get_collator(),  # 使用数据集的自定义collate函数\n",
    "    sampler=RandomSampler(train_dataset),     # 随机采样数据\n",
    "    batch_size=1,                             # 每次处理一个样本\n",
    ")\n",
    "\n",
    "print(f\"DataLoader 已创建，批次大小: 1\")\n",
    "print(f\"总批次数: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fc634",
   "metadata": {},
   "source": [
    "## DPO损失函数\n",
    "\n",
    "DPO的核心是比较策略模型和参考模型在更好/更差回答上的对数概率差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be039817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO损失函数已定义\n"
     ]
    }
   ],
   "source": [
    "def strip_pad(seq: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"移除序列中的填充token\"\"\"\n",
    "    return seq[seq != pad_token_id]\n",
    "\n",
    "def compute_log_probs(model, batch, tokenizer):\n",
    "    \"\"\"计算给定序列的对数概率\"\"\"\n",
    "    # 移除meta_info，只保留模型输入\n",
    "    model_inputs = {k: v for k, v in batch.items() if k != 'meta_info'}\n",
    "    \n",
    "    with torch.no_grad() if model == reference_model else torch.enable_grad():\n",
    "        logits = model(**model_inputs).logits\n",
    "    \n",
    "    device = logits.device\n",
    "    input_ids = batch['input_ids']\n",
    "    batch_size = len(batch['meta_info']['response_lens'])\n",
    "    logprob_list = []\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        response_length = batch['meta_info']['response_lens'][idx]\n",
    "        raw_input_id = strip_pad(input_ids[idx], tokenizer.pad_token_id)\n",
    "        logit = logits[idx][-response_length:].unsqueeze(0)\n",
    "        input_id = raw_input_id[-response_length:].unsqueeze(0)\n",
    "        log_p = gather_log_probabilities(logit[:, :-1], input_id[:, 1:])\n",
    "        logprob_list.append(log_p.squeeze(0))\n",
    "    \n",
    "    return torch.nn.utils.rnn.pad_sequence(\n",
    "        logprob_list, batch_first=True, padding_value=0.0\n",
    "    ).to(device)\n",
    "\n",
    "def dpo_loss(batch, scale_coeff=0.1):\n",
    "    \"\"\"计算DPO损失\"\"\"\n",
    "    # 计算策略模型的对数概率\n",
    "    sequence_log_probs = compute_log_probs(policy_model, batch, tokenizer)\n",
    "    better_sequence_log_probs, worse_sequence_log_probs = sequence_log_probs.chunk(chunks=2, dim=0)\n",
    "    \n",
    "    # 计算参考模型的对数概率\n",
    "    with torch.no_grad():\n",
    "        ref_sequence_log_probs = compute_log_probs(reference_model, batch, tokenizer)\n",
    "        ref_better_sequence_log_probs, ref_worse_sequence_log_probs = ref_sequence_log_probs.chunk(chunks=2, dim=0)\n",
    "    \n",
    "    losses = []\n",
    "    better_sample_rewards = []\n",
    "    worse_sample_rewards = []\n",
    "    \n",
    "    batch_size = better_sequence_log_probs.size(0)\n",
    "    for i in range(batch_size):\n",
    "        better_log_prob = better_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        worse_log_prob = worse_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        ref_better_log_prob = ref_better_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        ref_worse_log_prob = ref_worse_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        \n",
    "        better_log_ratio = better_log_prob - ref_better_log_prob\n",
    "        worse_log_ratio = worse_log_prob - ref_worse_log_prob\n",
    "        \n",
    "        losses.append(\n",
    "            -F.logsigmoid(scale_coeff * (better_log_ratio - worse_log_ratio))\n",
    "        )\n",
    "        better_sample_rewards.append(scale_coeff * better_log_ratio.detach())\n",
    "        worse_sample_rewards.append(scale_coeff * worse_log_ratio.detach())\n",
    "    \n",
    "    loss = torch.stack(losses).mean()\n",
    "    better_sample_reward = torch.stack(better_sample_rewards)\n",
    "    worse_sample_reward = torch.stack(worse_sample_rewards)\n",
    "    reward_accuracy = (better_sample_reward > worse_sample_reward).float().mean()\n",
    "    reward_margin = (better_sample_reward - worse_sample_reward).mean()\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'better_sample_reward': better_sample_reward.mean(),\n",
    "        'worse_sample_reward': worse_sample_reward.mean(),\n",
    "        'reward_accuracy': reward_accuracy,\n",
    "        'reward_margin': reward_margin,\n",
    "    }\n",
    "\n",
    "print(\"DPO损失函数已定义\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390928c",
   "metadata": {},
   "source": [
    "## DPO训练循环\n",
    "\n",
    "现在我们开始DPO训练。我们将训练几个epoch并监控各种指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1650980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/3000 [00:00<?, ?it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始DPO训练，共 3 个epoch\n",
      "缩放系数: 0.1\n",
      "模型保存路径: ./qwen_2_5_dpo_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 1/3000 [00:00<43:10,  1.16it/s]/root/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/root/miniconda3/lib/python3.12/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch 1/3:  33%|███▎      | 1000/3000 [02:07<04:22,  7.63it/s, loss=0.6936, acc=0.354, margin=0.0011, lr=5.00e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 完成:\n",
      "  平均损失: 0.6936\n",
      "  平均奖励准确率: 0.354\n",
      "  平均奖励边际: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  33%|███▎      | 1001/3000 [02:09<26:08,  1.27it/s, loss=0.6936, acc=0.354, margin=0.0011, lr=5.00e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  模型已保存到: ./qwen_2_5_dpo_output/epoch_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  67%|██████▋   | 2000/3000 [04:11<01:54,  8.77it/s, loss=0.6895, acc=0.399, margin=0.0095, lr=5.00e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 完成:\n",
      "  平均损失: 0.6854\n",
      "  平均奖励准确率: 0.443\n",
      "  平均奖励边际: 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  67%|██████▋   | 2001/3000 [04:13<11:32,  1.44it/s, loss=0.6895, acc=0.399, margin=0.0095, lr=5.00e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  模型已保存到: ./qwen_2_5_dpo_output/epoch_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 3000/3000 [06:20<00:00,  8.36it/s, loss=0.6872, acc=0.421, margin=0.0144, lr=5.00e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 完成:\n",
      "  平均损失: 0.6827\n",
      "  平均奖励准确率: 0.465\n",
      "  平均奖励边际: 0.0242\n",
      "  模型已保存到: ./qwen_2_5_dpo_output/epoch_3\n",
      "\n",
      "DPO训练完成！\n"
     ]
    }
   ],
   "source": [
    "# 训练配置\n",
    "epochs = 3\n",
    "scale_coeff = 0.1  # DPO的缩放系数\n",
    "window_size = 100  # 指标统计step间隔\n",
    "save_dir = './qwen_2_5_dpo_output'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 训练指标记录\n",
    "progress_bar = tqdm(range(epochs * len(train_dataloader)), desc=\"DPO Training\")\n",
    "losses = []\n",
    "reward_accuracies = []\n",
    "reward_margins = []\n",
    "\n",
    "\n",
    "print(f\"开始DPO训练，共 {epochs} 个epoch\")\n",
    "print(f\"缩放系数: {scale_coeff}\")\n",
    "print(f\"模型保存路径: {save_dir}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    progress_bar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    policy_model.train()\n",
    "\n",
    "    epoch_step = 0\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    epoch_margins = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # 计算DPO损失\n",
    "        loss_dict = dpo_loss(batch, scale_coeff=scale_coeff)\n",
    "        loss = loss_dict['loss']\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录指标\n",
    "        epoch_step += 1\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accuracies.append(loss_dict['reward_accuracy'].item())\n",
    "        epoch_margins.append(loss_dict['reward_margin'].item())\n",
    "\n",
    "        if epoch_step % window_size == 0:\n",
    "            losses.append(np.mean(epoch_losses[-window_size:]))\n",
    "            reward_accuracies.append(np.mean(epoch_accuracies[-window_size:]))\n",
    "            reward_margins.append(np.mean(epoch_margins[-window_size:]))\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # 更新进度条显示\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{np.mean(losses):.4f}\",\n",
    "            'acc': f\"{np.mean(reward_accuracies):.3f}\",\n",
    "            'margin': f\"{np.mean(reward_margins):.4f}\",\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "        \n",
    "    # Epoch结束后的统计\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    avg_accuracy = np.mean(epoch_accuracies)\n",
    "    avg_margin = np.mean(epoch_margins)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} 完成:\")\n",
    "    print(f\"  平均损失: {avg_loss:.4f}\")\n",
    "    print(f\"  平均奖励准确率: {avg_accuracy:.3f}\")\n",
    "    print(f\"  平均奖励边际: {avg_margin:.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    epoch_save_dir = os.path.join(save_dir, f'epoch_{epoch+1}')\n",
    "    os.makedirs(epoch_save_dir, exist_ok=True)\n",
    "    \n",
    "    policy_model.save_pretrained(epoch_save_dir)\n",
    "    tokenizer.save_pretrained(epoch_save_dir)\n",
    "    \n",
    "    print(f\"  模型已保存到: {epoch_save_dir}\")\n",
    "\n",
    "print(\"\\nDPO训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fd64a",
   "metadata": {},
   "source": [
    "## 训练结果分析\n",
    "\n",
    "让我们分析训练过程中的指标变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273277d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制训练指标\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "# 损失曲线\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_title('DPO Loss')\n",
    "axes[0].set_xlabel('Steps')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 奖励准确率\n",
    "axes[1].plot(reward_accuracies)\n",
    "axes[1].set_title('Reward Accuracy')\n",
    "axes[1].set_xlabel('Steps')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# 奖励边际\n",
    "axes[2].plot(reward_margins)\n",
    "axes[2].set_title('Reward Margin')\n",
    "axes[2].set_xlabel('Steps')\n",
    "axes[2].set_ylabel('Margin')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, 'training_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9fc80",
   "metadata": {},
   "source": [
    "## 模型测试\n",
    "\n",
    "让我们测试训练后的模型，看看它的表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f767342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 模型测试 ===\n",
      "\n",
      "比较训练前后的模型回答：\n",
      "\n",
      "问题 1: Please explain what machine learning is?\n",
      "--------------------------------------------------\n",
      "参考模型回答: Machine Learning (ML) is an interdisciplinary field that focuses on the development of algorithms and statistical models that computer systems can use to enable computers to learn from and improve from data without being explicitly programmed. This means that ML allows machines to \"learn\" from experience or data in order to make predictions or decisions based on those experiences.\n",
      "\n",
      "Key aspects of Machine Learning include:\n",
      "\n",
      "1. **Data**: The raw information used for training a model.\n",
      "2. **Model**: A set of rules or equations that describes how the model learns from its training data.\n",
      "3. **Algorithm**: A specific type of rule-based model designed to solve a particular problem.\n",
      "4. **Evaluation**: A way to measure the performance of a model against real-world data.\n",
      "5. **Training**: The process where the algorithm learns from the given data.\n",
      "6. **Testing**: The evaluation phase after the initial training period to see if the learned model performs well on new data.\n",
      "7. **Deployment**: Where the model is applied to real-world situations to achieve desired results.\n",
      "\n",
      "Machine Learning is widely used across various industries such as finance, healthcare, marketing, and more. It's particularly useful when dealing with large datasets that might be difficult to analyze directly using traditional methods like supervised learning or unsupervised learning. By leveraging big data and advanced computational power, ML enables systems to learn patterns and insights from complex data sets, which can then be used for predictive analytics, recommendation systems, and other applications.\n",
      "\n",
      "训练后模型回答: Machine learning (ML) refers to the study of algorithms and statistical models that computer systems can use to enable computers to learn from and improve from data. It involves training models on large datasets to identify patterns, make predictions, or solve problems without being explicitly programmed.\n",
      "\n",
      "In simpler terms, machine learning allows computers to learn from experience and adapt their behavior based on new inputs. It uses algorithms to analyze data and extract insights from it. The goal is to create machines that can perform tasks with minimal human intervention and often in real-time situations. Machine learning techniques are widely used in various fields such as healthcare, finance, robotics, and more.\n",
      "================================================================================\n",
      "\n",
      "问题 2: How to learn programming?\n",
      "--------------------------------------------------\n",
      "参考模型回答: Learning programming can be an exciting and rewarding experience! Here are some steps you can follow to get started:\n",
      "\n",
      "1. Choose the right language: There are many programming languages available, such as Python, Java, C++, JavaScript, Ruby, and more. Choose one that aligns with your interests and goals.\n",
      "\n",
      "2. Learn the basics: Start with simple concepts like variables, data types, control structures (if/else statements), loops, functions, and basic object-oriented programming (OOP). These foundational skills will help you understand the syntax of different programming languages.\n",
      "\n",
      "3. Practice regularly: The best way to learn is through practice. Write code snippets in various programming languages and try to solve problems on leetcode or any other platform.\n",
      "\n",
      "4. Build projects: Use your knowledge to build small projects or programs. This helps reinforce what you've learned and also teaches you how to think logically and creatively.\n",
      "\n",
      "5. Join communities: Participate in online forums, meetups, and coding groups where you can connect with others who share your interest in programming. They can provide guidance, feedback, and support.\n",
      "\n",
      "6. Read books: If possible, read books about computer science and programming. They can help deepen your understanding of programming concepts and give you practical examples.\n",
      "\n",
      "7. Take courses: Some universities offer free online courses that teach programming. Online platforms like Coursera, Udemy, and edX offer numerous courses from top institutions around the world.\n",
      "\n",
      "8. Learn from experts: Look up resources such as blogs, videos, and podcasts written by experienced programmers. They can provide insights into best practices, tutorials, and tips.\n",
      "\n",
      "Remember, learning programming takes time and dedication. Be patient with yourself, and don't hesitate to ask for help when needed. Good luck on your journey to becoming a proficient programmer!\n",
      "\n",
      "训练后模型回答: Learning programming can be approached in several ways, and the best approach for you may depend on your learning style and goals. Here are some general steps that you might consider:\n",
      "\n",
      "1. Choose an introductory language: If you're new to programming, it's generally recommended to start with a beginner-level language like Python or JavaScript.\n",
      "\n",
      "2. Build projects: Once you have an understanding of basic concepts, building small projects is a great way to reinforce what you've learned and gain practical experience.\n",
      "\n",
      "3. Read books and tutorials: There are many resources available online that provide step-by-step instructions for programming languages. Books like \"Python Crash Course\" or \"The Art of Computer Programming\" can be very helpful.\n",
      "\n",
      "4. Use online courses: Platforms like Coursera, Udemy, and Khan Academy offer a wide range of free and paid courses in computer science, including programming languages.\n",
      "\n",
      "5. Join a community: Online forums such as Reddit's r/learnprogramming or Stack Overflow can provide support and help you troubleshoot problems.\n",
      "\n",
      "6. Practice coding regularly: The more you code, the better you'll become at it. Set aside time each day or week to work on coding challenges or projects.\n",
      "\n",
      "7. Learn from real-world examples: Watch videos, read articles, and practice writing code snippets related to real-world applications.\n",
      "\n",
      "8. Consider taking a course at a local university or college: Many universities offer courses specifically for beginners in computer science.\n",
      "\n",
      "Remember, learning any new skill takes time and effort. Be patient with yourself, and don't be discouraged if progress seems slow at first. With consistent practice and dedication, you'll likely develop strong skills in programming over time.\n",
      "================================================================================\n",
      "\n",
      "问题 3: What is deep learning?\n",
      "--------------------------------------------------\n",
      "参考模型回答: Deep learning is an interdisciplinary field that combines the principles of machine learning with the architecture and structure of the human brain to enable computers to learn from large amounts of data without being explicitly programmed for every possible outcome.\n",
      "\n",
      "In simple terms, deep learning involves training artificial neural networks (the \"neurons\" in their simplest form) on complex datasets such as images, text, or audio. These networks are then able to make predictions based on patterns and relationships within the data. The key components of a deep learning model include layers, which act as nodes that allow information to be passed between neurons; activation functions that adjust the weight of connections based on the input data; and backpropagation, which adjusts these weights during training to minimize errors between predicted outputs and actual outcomes.\n",
      "\n",
      "Deep learning has been applied across various domains, including image recognition, speech processing, natural language processing, computer vision, and robotics, among others. It's particularly well-suited for tasks involving high-dimensional data, where traditional machine learning algorithms struggle due to computational constraints. Deep learning models can automatically detect patterns and relationships that might not be apparent to humans, making them powerful tools for solving complex problems in fields like finance, healthcare, and autonomous systems.\n",
      "\n",
      "训练后模型回答: Deep learning is a subfield of artificial intelligence that involves the development of algorithms and models inspired by the structure and function of the human brain. It uses neural networks to process and analyze large amounts of data, allowing computers to learn from raw input without being explicitly programmed.\n",
      "\n",
      "Key characteristics of deep learning include:\n",
      "\n",
      "1. **Complexity**: Deep learning can model complex patterns in data due to its reliance on interconnected layers of neurons.\n",
      "2. **Parallel Processing**: Unlike classical machine learning methods where each node performs computations sequentially, deep learning processes information in parallel across multiple nodes (neurons).\n",
      "3. **Hierarchical Structure**: Neural networks often have hierarchical structures, enabling them to capture higher-level features and relationships within data.\n",
      "4. **Recurrent Structures**: Many deep learning architectures incorporate mechanisms for recurrent processing, which allows them to handle sequential data like time series or sequences.\n",
      "5. **Feature Extraction**: Techniques such as convolutional neural networks (CNNs) extract features from low-level images and features extracted from high-level images are used in deep learning for image recognition tasks.\n",
      "\n",
      "Applications of deep learning include:\n",
      "\n",
      "- Image and video recognition\n",
      "- Speech recognition\n",
      "- Natural language processing\n",
      "- Robotics\n",
      "- Computer vision\n",
      "- Machine translation\n",
      "\n",
      "Despite its wide-ranging applications, deep learning has also faced challenges related to scalability, interpretability, and ethical considerations.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=512):\n",
    "    \"\"\"使用模型生成回答\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 格式化输入\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 编码输入\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # 生成回答\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# 测试问题\n",
    "test_prompts = [\n",
    "    \"Please explain what machine learning is?\",\n",
    "    \"How to learn programming?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Please introduce the Python programming language.\",\n",
    "    \"How to improve writing skills?\"\n",
    "]\n",
    "\n",
    "print(\"=== 模型测试 ===\")\n",
    "print(\"\\n比较训练前后的模型回答：\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts[:3]):  # 只测试前3个问题\n",
    "    print(f\"问题 {i+1}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 参考模型（训练前）的回答\n",
    "    ref_response = generate_response(reference_model, tokenizer, prompt)\n",
    "    print(f\"参考模型回答: {ref_response}\")\n",
    "    print()\n",
    "    \n",
    "    # 策略模型（训练后）的回答\n",
    "    policy_response = generate_response(policy_model, tokenizer, prompt)\n",
    "    print(f\"训练后模型回答: {policy_response}\")\n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
