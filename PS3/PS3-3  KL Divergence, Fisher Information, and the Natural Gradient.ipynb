{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PS3-3 KL Divergence, Fisher Information, and the Natural Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    E_{y\\sim p(y,\\theta)}[\\nabla_{\\theta}\\log p(y;\\theta)] &= \\int p(y;\\theta) \\frac{\\nabla_{\\theta}p(y;\\theta)}{p(y;\\theta)}\\\\\n",
    "    & = \\int \\nabla_{\\theta}p(y;\\theta) = \\nabla_{\\theta} \\int p(y;\\theta) \\\\\n",
    "    & = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    Cov[X] & = E[(X-E[X])(X-E[X])^T] \\\\\n",
    "    & = E[XX^T] \\hspace{1em} when \\ E[X] = 0 \\\\\n",
    "    I(\\theta) &= Cov[\\nabla_{\\theta}\\log p(y;\\theta)] \\\\\n",
    "    & = E[\\nabla_{\\theta}\\log p(y;\\theta) \\nabla_{\\theta}\\log p(y;\\theta)^T]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\nabla_{\\theta}^2[p(y;\\theta)] & = \\nabla_{\\theta}[\\frac{\\nabla_{\\theta}p(y;\\theta)}{p(y;\\theta)}p(y;\\theta)] \\\\\n",
    "    & = \\nabla_{\\theta}(\\frac{\n",
    "    \\nabla_{\\theta}p(y;\\theta)}{p(y;\\theta)})p(y;\\theta) + \\frac{\\nabla_{\\theta}p(y;\\theta)}{p(y;\\theta)} \\nabla_{\\theta} p(y;\\theta)^T \\\\\n",
    "    & = \\nabla_{\\theta}^2 \\log p(y;\\theta) p(y;\\theta)+ \\nabla_{\\theta} \\log p(y;\\theta) \\nabla_{\\theta}\\log p(y;\\theta)^T p(y;\\theta)\n",
    "\\end{align*}\n",
    "两边对y求积分即得：\n",
    "\\begin{align*}\n",
    "    E [\\nabla_{\\theta}\\log p(y;\\theta) \\nabla_{\\theta}\\log p(y;\\theta)^T] = E[-\\nabla_{\\theta} ^2 \\log p(y;\\theta)]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接讲taylor公式代入，再用第二问推出来的公式,以及第一问的score function期望为0，即可得到\n",
    "\\begin{align*}\n",
    "    D_{KL}(p_{\\theta} || p(\\theta+d)) \\sim \\frac{1}{2}d^TI(\\theta)d\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这题就是让你在约束条件\n",
    "$$ D_{KL}(p_{\\theta}||p_{\\theta+d}) = c $$\n",
    "下，去找d使得 $ l(\\theta+d)$ 的值最大，所以很自然的就考虑拉格朗日乘子法\n",
    "再利用估计\n",
    "\\begin{align*}\n",
    "    l(\\theta + d) &\\sim l(\\theta) + d^T\\nabla_{\\theta} l(\\theta) \\\\\n",
    "    D_{KL}(p_\\theta || p_(\\theta + d)) &\\sim \\frac{1}{2}d^TI(\\theta)d \\\\\n",
    "    L(d;\\lambda) &= l(\\theta+d) - \\lambda (D_{KL}(p(\\theta) || p(\\theta +d))-c)\n",
    "\\end{align*}\n",
    "对$\\theta$求偏导可得：\n",
    "$$d = \\frac{1}{\\lambda} I(\\theta)^{-1} \\frac{\\nabla_{\\theta}p(y;\\theta)}{p(y;\\theta)} $$\n",
    "后面解$\\lambda$比较冗长就不写了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method\n",
    "$$ \\theta:= \\theta - H^{-1}\\nabla_{\\theta}l(\\theta)$$\n",
    "Natural's gradient\n",
    "\\begin{align*}\n",
    "    I(\\theta) &= E(-\\nabla_{\\theta}) = -E(H)\\\\\n",
    "    d &= -\\frac{1}{\\lambda} E(H)^{-1} \\nabla_{\\theta} l(\\theta)\n",
    "\\end{align*}\n",
    "可以看到两者是几乎一致的"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
